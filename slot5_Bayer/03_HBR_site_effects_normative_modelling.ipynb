{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d09a59a-310b-4565-95f5-1f3e9721fdec",
   "metadata": {},
   "source": [
    "# Normative modelling as a site effect correction tool\n",
    "\n",
    "This tutorial will walk you through the application of normative modelling as a site effect correction tool.\n",
    "\n",
    "You will learn:\n",
    "- How to fit a normative model to data from different sites.\n",
    "- Choices that can be made about a normative model in a site effect context.\n",
    "- How to transfer data from one site into another.\n",
    "- How to make predictions for unseen sites and the rationale behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaac9b3-3ad6-47ee-a06f-528f7a0c0aae",
   "metadata": {},
   "source": [
    "# Quick recap: normative modelling using the pcntoolkit\n",
    "A normative model requires three input variables:\n",
    "- covariates (the X axis, ususally age)\n",
    "- response variables (usually one or several brain feature measurements)\n",
    "- a batch effect file. Batch effects can be anything that interferes with the covrariate (sex, site)\n",
    "\n",
    "While the Bayesian Linear Regression (BLR) model requires input of the batch effects via a design matrix, the implementation of the Hierarchical Bayesian Regression (HBR) model only requires a text file (vector) - the design matrix is created internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0364e4-ac5b-4f19-8aef-3c7b5b7d7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the toolkit\n",
    "!pip install pcntoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5248840-9b00-4820-8593-85d9d7ad4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import pcntoolkit as ptk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2530d9-bb2a-48b6-ad79-fc01842953db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple function to quickly load pickle files\n",
    "def ldpkl(filename: str):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c07f21-42ea-4645-b701-ef9c028098c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick check wehere you are in your directory.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103186c8-97aa-49e4-a6d1-72daac14d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create working dir\n",
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f2874-4006-4703-ad49-17ff3f0c20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae41f1b-5a89-4e6a-b65f-38f09510cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_folder = \"Site_effect_tutorial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33d09a-63f4-48b0-b609-ecdbbccba52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(processing_folder):\n",
    "    os.makedirs(processing_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77debe6f-3a1f-4f8f-9d99-99a6dba6718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(processing_folder)\n",
    "processing_dir = os.getcwd()\n",
    "print(f\"The processing directory is: {processing_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639454a-4bc9-4b23-bed1-7baadfb9f3b5",
   "metadata": {},
   "source": [
    "# Step 1: Data pre-processing\n",
    "\n",
    "### 1.1 Load the data\n",
    "\n",
    "For this tutorial, we use publicly availabe data that has been pooled from various data sets. The data have alrrady been conveniently split into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08aa693-1a88-416e-8012-ce3d1ca008dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "!wget -nc https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_tr.csv\n",
    "!wget -nc https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_te.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3837b-b424-48a2-8416-1505f50c845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data into the notebook\n",
    "test_data = os.path.join(processing_dir, 'fcon1000_te.csv')\n",
    "df_te = pd.read_csv(test_data, index_col=0)\n",
    "\n",
    "# extract a list of unique site ids from the test set\n",
    "site_ids_te =  sorted(set(df_te['site'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a2cc6-721d-4a41-b5c7-6577715ab522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data into the notebook\n",
    "train_data = os.path.join(processing_dir, 'fcon1000_tr.csv')\n",
    "\n",
    "df_tr = pd.read_csv(train_data, index_col=0)\n",
    "\n",
    "# extract a list of unique site ids from the test set\n",
    "site_ids_ad =  sorted(set(df_tr['site'].to_list()))\n",
    "\n",
    "if not all(elem in site_ids_ad for elem in site_ids_te):\n",
    "    print('Warning: some of the testing sites are not in the adaptation data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0374f4-58a1-41ee-b8fc-9a8202152b5c",
   "metadata": {},
   "source": [
    "### 1.2 Visual inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37856974-a5c2-4e9c-abd2-aab2338071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a quick and dirty plot of the age distibution per site.\n",
    "axes = df_tr.hist(['age'], by='site', layout=(5,5),\n",
    "                  legend=True, yrot=90,sharex=True,sharey=True, \n",
    "                  figsize=(6,6))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlabel('age')\n",
    "    ax.set_ylabel('distribution')\n",
    "    ax.set_ylim(bottom=1,top=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5085f-977d-4739-ab27-c8856da0037e",
   "metadata": {},
   "source": [
    "We see that we have 22 sites (ids: 0-21) in the data set. \n",
    "Additionally, let's now print the number of subjects per site for the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08ae6f-90c2-4bba-9d81-4c46c1486be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df_tr['site'].unique()\n",
    "\n",
    "print('sample size check')\n",
    "for i,s in enumerate(sites):\n",
    "    idx = df_tr['site'] == s\n",
    "    idxte = df_te['site'] == s\n",
    "    print(i,s, sum(idx), sum(idxte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d410c87-a944-4294-bada-39db20a6a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['sitenum'] = 0\n",
    "for i,s in enumerate(sites):\n",
    "    idx = df_tr['site'] == s\n",
    "    df_tr['sitenum'].loc[idx] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d05f5c-a9c0-427a-a673-88e7f1d1435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te['sitenum'] = 0\n",
    "for i,s in enumerate(sites):\n",
    "    idx = df_te['site'] == s\n",
    "    df_te['sitenum'].loc[idx] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e32cf1-b2c0-4fea-beec-d3034d933086",
   "metadata": {},
   "source": [
    "### 1.3 Set aside of hold-out transfer sites\n",
    "For our site transfer later we want to exclude some sites from the training and test set for later. We choose the SaintLois and the COBRE site and remove them from the training and test set. We summarize those sites in a `new_sites_tr` and `new_sites_te` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a0979-6606-47b4-aebd-c6e16516f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some sites for later, these are going to be my new sites\n",
    "new_sites_tr = df_tr[(df_tr['site']== 'SaintLouis')| (df_tr['site']=='COBRE')]\n",
    "new_sites_te = df_te[(df_te['site']== 'SaintLouis')| (df_te['site']=='COBRE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472388f-fa1c-4c14-a712-7d935ad3ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a quick look at the shape of the held out data:\n",
    "new_sites_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a4ccb-5d2e-40c4-8c84-8c1c926bb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove those sites from the training and test sets\n",
    "df_tr = df_tr[df_tr.site != 'SaintLouis']\n",
    "df_tr = df_tr[df_tr.site != 'COBRE']\n",
    "df_te = df_te[df_te.site != 'SaintLouis']\n",
    "df_te = df_te[df_te.site != 'COBRE']\n",
    "# make sure we have only those 20 intended sites in the training set:\n",
    "len(df_tr[\"site\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dce878-c4bb-4855-b4d0-40c28a95ee8c",
   "metadata": {},
   "source": [
    "### 1.4 Select brain variables (independently derived phenotypes, IDPs)\n",
    "\n",
    "Our data set contains 223 columns (phenotypes). We select two phenotypes for modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195228da-3046-411d-872b-a0f6e2b2fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the IDPs (columns) of interest from the data frame\n",
    "idps = ['lh_G&S_frontomargin_thickness','rh_G&S_frontomargin_thickness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a59ef4-3a45-4467-88ba-c8cd87e034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data frames for training and testing\n",
    "\n",
    "X_train = (df_tr['age']/100).to_numpy(dtype=float) # we rescale age to range between [0,1]\n",
    "Y_train = df_tr[idps].to_numpy(dtype=float)\n",
    "batch_effects_train = df_tr[['sitenum', 'sex']].to_numpy(dtype=int)\n",
    "\n",
    "# save data\n",
    "with open('X_train.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_train), file)\n",
    "with open('Y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(Y_train), file)\n",
    "with open('trbefile.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(batch_effects_train), file)\n",
    "\n",
    "\n",
    "X_test = (df_te['age']/100).to_numpy(dtype=float)\n",
    "Y_test = df_te[idps].to_numpy(dtype=float)\n",
    "batch_effects_test = df_te[['sitenum', 'sex']].to_numpy(dtype=int)\n",
    "\n",
    "#save data\n",
    "with open('X_test.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_test), file)\n",
    "with open('Y_test.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(Y_test), file)\n",
    "with open('tsbefile.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(batch_effects_test), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafbcac-9518-4a0b-8efb-85c8be440328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model requires the paths to the data. We reate them here.\n",
    "\n",
    "respfile = os.path.join(processing_dir, 'Y_train.pkl')       # measurements  (eg cortical thickness) of the training samples (columns: the various features/ROIs, rows: observations or subjects)\n",
    "covfile = os.path.join(processing_dir, 'X_train.pkl')        # covariates (eg age) the training samples (columns: covariates, rows: observations or subjects)\n",
    "\n",
    "testrespfile_path = os.path.join(processing_dir, 'Y_test.pkl')       # measurements  for the testing samples\n",
    "testcovfile_path = os.path.join(processing_dir, 'X_test.pkl')        # covariate file for the testing samples\n",
    "\n",
    "trbefile = os.path.join(processing_dir, 'trbefile.pkl')      # training batch effects file (eg scanner_id, gender)  (columns: the various batch effects, rows: observations or subjects)\n",
    "tsbefile = os.path.join(processing_dir, 'tsbefile.pkl')      # testing batch effects file\n",
    "\n",
    "output_path = os.path.join(processing_dir, 'Models/')    #  output path, where the models will be written\n",
    "log_dir = os.path.join(processing_dir, 'log/')           # log path\n",
    "if not os.path.isdir(output_path):\n",
    "    os.mkdir(output_path)\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "outputsuffix = 'estimate'      # a string to name the output files, of use only to you, so adapt it for your needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcff7978-0dbf-41b8-8b83-fa17eed41e55",
   "metadata": {},
   "source": [
    "# Step 2: Normative modelling\n",
    "\n",
    "\n",
    "## Case 1: Training a normative model on different sites to make predictions on the same sites (equal distribution of batches across training and test data)\n",
    "\n",
    "\n",
    "Use: Hierarchical Bayesian model (HBR)\n",
    "\n",
    "HBR multi-batch normative modelling class.\n",
    "   \n",
    "    :param X: [N×P] array of clinical covariates\n",
    "    :param y: [N×1] array of neuroimaging measures\n",
    "    :param trbefile: the address to the batch effects file for the training set.\n",
    "        the batch effect array should be a [N×M] array where M is the number of\n",
    "        the type of batch effects. For example when the site and gender is modeled\n",
    "        as batch effects M=2. Each column in the batch effect array contains the\n",
    "        batch ID (starting from 0) for each sample. If not specified (default=None)\n",
    "        then all samples assumed to be from the same batch (i.e., the batch effect\n",
    "                                                           is not modelled).\n",
    "    :param tsbefile: Similar to trbefile for the test set.\n",
    "    :param model_type: Specifies the type of the model from 'linear', 'plynomial',\n",
    "          and 'bspline' (defauls is 'linear').\n",
    "    :param likelihood: specifies the type of likelihood among 'Normal' 'SHASHb','SHASHo',\n",
    "       and 'SHASHo2' (defauls is normal).\n",
    "    :param linear_mu: Boolean (default='True') to decide whether the mean (mu) is\n",
    "        parametrized on a linear function (thus changes with covariates) or it is fixed.\n",
    "    :param linear_sigma: Boolean (default='False') to decide whether the variance (sigma) is\n",
    "        parametrized on a linear function (heteroscedastic noise) or it is fixed for\n",
    "        each batch (homoscedastic noise).\n",
    "    :param linear_epsilon: Boolean (default='False') to decide the parametrization\n",
    "        of epsilon for the SHASH likelihood that controls its skewness.\n",
    "        If True, epsilon is  parametrized on a linear function\n",
    "       (thus changes with covariates) otherwise it is fixed for each batch.\n",
    "    :param linear_delta: Boolean (default='False') to decide the parametrization\n",
    "        of delta for the SHASH likelihood that controls its kurtosis.\n",
    "        If True, delta is  parametrized on a linear function\n",
    "        (thus changes with covariates) otherwise it is fixed for each batch.\n",
    "    :param random_intercept_{parameter}: if parameters mu (default='True'),\n",
    "        sigma (default='False'), epsilon (default='False'), and delta (default='False')\n",
    "        are parametrized on a linear function, then this boolean decides\n",
    "        whether the intercept can vary across batches.\n",
    "    :param random_slope_{parameter}: if parameters mu (default='True'),\n",
    "        sigma (default='False'), epsilon (default='False'), and delta (default='False')\n",
    "        are parametrized on a linear function, then this boolean decides\n",
    "        whether the slope can vary across batches.\n",
    "    :param centered_intercept_{parameter}: if parameters mu (default='False'),\n",
    "        sigma (default='False'), epsilon (default='False'), and delta (default='False')\n",
    "        are parametrized on a linear function, then this boolean decides\n",
    "        whether the parameters of intercept are estimated in a centered or\n",
    "        non-centered manner (default). While centered estimation runs faster\n",
    "        it may cause some problems for the sampler (the funnel of hell).\n",
    "    :param centered_slope_{parameter}: if parameters mu (default='False'),\n",
    "        sigma (default='False'), epsilon (default='False'), and delta (default='False')\n",
    "        are parametrized on a linear function, then this boolean decides\n",
    "        whether the parameters of slope are estimated in a centered or\n",
    "        non-centered manner (default). While centered estimation runs faster\n",
    "        it may cause some problems for the sampler (the funnel of hell).\n",
    "    :param sampler: specifies the type of PyMC sampler (Defauls is 'NUTS').\n",
    "    :param n_samples: The number of samples to draw (Default is '1000'). Please\n",
    "        note that this parameter must be specified in a string fromat ('1000' and\n",
    "                                                                      not 1000).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa3835-d8f5-41f8-afe9-e548e3e963c5",
   "metadata": {},
   "source": [
    "### Case 1: Selecting the model\n",
    "\n",
    "The model framework for this tutorial is the hierarchial Bayesian Regression model (HBR).\n",
    "\n",
    "#### Choices we make for the model:\n",
    "- We decide for our model that we want the `mean` and the `variance` to vary with the covariate (`linear_mu` and `linear_sigma`, respectively). We choose a non-linear function for $\\phi$ given the non-linear change of age with some brain variables. We choose a b-spline basis expansion.\n",
    "\n",
    "- However, given that our dataset is quite small, we decide to use a model with a normal likelihood (the default). An example of a more complex likelihod and its application can be found [here](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/HBR_SHASH/HBR_Tutorial.ipynb).\n",
    "\n",
    "- Given the many sites in the model (and for demonstration purposes), we want the model to be rather flexible and the centiles 'wiggly'. We choose a random intercept offsets and slopes for $\\mu$ and $\\sigma$. You can change these parameters to your liking and see what happens. \n",
    "\n",
    "#### Files required:\n",
    "- The default version of the model always trains the model on a training set, and then makes predictions on a test set.\n",
    "- Hence we need: a full training set (covariates, responses, batch effects) and test set (covariates, responses, batch effects). All batch effects need to be distributed rather equally across training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c806a-e4f1-4798-8e24-a1ff8b010dd0",
   "metadata": {},
   "source": [
    "### Case 1: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff2cc7-8102-4157-be8f-b915da827955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "ptk.normative.estimate(covfile=covfile, \n",
    "                       respfile=respfile,\n",
    "                       tsbefile=tsbefile, \n",
    "                        model_type='bspline',\n",
    "                       linear_mu='True',\n",
    "                       linear_sigma = 'True',\n",
    "                       random_intercept_sigma = 'True',\n",
    "                       random_slope_sigma = 'True',\n",
    "                       random_intercept_mu='True',\n",
    "                       random_slope_mu='True',\n",
    "                       alg='hbr', \n",
    "                       log_path=log_dir, \n",
    "                       binary=True,\n",
    "                       output_path=output_path, \n",
    "                       testcov= testcovfile_path,\n",
    "                       testresp = testrespfile_path,\n",
    "                       trbefile=trbefile,\n",
    "                       outputsuffix=outputsuffix, \n",
    "                       savemodel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e6828-de3e-4f9b-b9ee-254fcf9e3d15",
   "metadata": {},
   "source": [
    "### Case 1: Load the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94234b-fc23-4c6c-89f0-add4a1f54cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the estimated model for the first IDP (column). \n",
    "model1_estimate = ldpkl(os.path.join(processing_dir, \"Models/NM_0_0_estimate.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00434b7-b388-4019-aa4f-1dd7f0fb2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can always have a display of all available methods using the help call:\n",
    "help(model1_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995b7e7-4a8d-43d2-a81c-7cab230b8c0e",
   "metadata": {},
   "source": [
    "### Case 1: Plot the results for one site\n",
    "\n",
    "The results yield predictions for all 20 sites. However, let's plot just now the results and centiles for site id 8 (Cleveland).\n",
    "\n",
    "We can use the `get_mcmc_quantiles` function inherent in the normative model object for that.\n",
    "This function requires:\n",
    "- a desired range of z-scores that the quantiles should cover\n",
    "- a desired X-axis vector (for example, from 20 years - 80 years)\n",
    "- a batch effect vector containing the desired batch effects to plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3803c5-50cd-4f11-a922-84e65c5fa7b2",
   "metadata": {},
   "source": [
    "### Case 1: Prepare dummy data and batch matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7585f5-200e-4481-94dd-d793b78bd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy zscores\n",
    "zscores = np.arange(-3,4)[:,np.newaxis]\n",
    "\n",
    "# We need to provide a matrix of shape [N,d]. In this case d=1, so we need to expand this matrix with a single 'empty' dimension.\n",
    "# We can do this by using the np.newaxis in this way.\n",
    "# Create sythetic X achsis\n",
    "n_synthetic_samples = 200\n",
    "synthetic_X = np.linspace(0.15, 0.85, n_synthetic_samples)[:,np.newaxis]\n",
    "# choose type of site effect by cerating intercept offset (or not). We choose gender 0 and site 8.\n",
    "be = np.zeros((n_synthetic_samples,2))\n",
    "sevens = np.ones((n_synthetic_samples,1))*8\n",
    "be[:,0] = sevens[:,0]\n",
    "be=be.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9edc17-37c0-4012-8622-2f2a3319c19e",
   "metadata": {},
   "source": [
    "### Case 1: Get the quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fdf51d-7074-49c6-8aa2-f44a585e0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantiles based on x achsis, batch effetcs be and zscore range\n",
    "q = model1_estimate.get_mcmc_quantiles(synthetic_X, be, zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086050b-27e1-49d6-9385-a3ada654be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training set data points (site 0) into the centiles\n",
    "plt.scatter(X_test[batch_effects_test[:,0]==8], Y_test[batch_effects_test[:,0]==8, 0])\n",
    "# Note: for illustrative reasons, we plot both gender 0 and 1 onto the centiles for gender 0. In a real analysis, one would probably \n",
    "# make one plot per gender.\n",
    "for i, v in enumerate(zscores):\n",
    "    thickness = 1\n",
    "    linestyle = \"-\"\n",
    "    if v == 0:\n",
    "        thickness = 2\n",
    "    if abs(v) > 2:\n",
    "        linestyle = \"--\"\n",
    "    plt.plot(synthetic_X, q[i], linewidth = thickness, linestyle = linestyle, color = 'black', alpha = 0.7)\n",
    "\n",
    "plt.title('site ' + str(be[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496923cb-72d2-4fa9-8122-d02693e91b05",
   "metadata": {},
   "source": [
    "## Case 2: Add data points from site 7 (Cambridge_Buckner) into the quantile plot of site 8 (Cleveland)\n",
    "\n",
    "We have now plotted the cebtiles for site 8 and plotted the raw data from site 8 in it. How can we add data from another site?\n",
    "\n",
    "1. Create a forward prediction for the covariate combination of each individual for site 7 for site 8.\n",
    "2. Use yhat and s2 to invert z score for those individuals.\n",
    "\n",
    "Rationale:\n",
    "\n",
    "The zscores that the model produces are normalized for site. Hence, the z-socres are comparable in z-score space.\n",
    "In order to convert the data points back into different site spaces, we need to revert the z-score transformation.\n",
    "\n",
    "### Some (rather informal) math\n",
    "\n",
    "$z_{7} = z_8$\n",
    "\n",
    "$z_7 = \\frac{\\hat{y}_8 - y_*}{s_8}$\n",
    "\n",
    "$z_7 * s_8 - \\hat{y_8} = -y_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a893a6-4b23-4cf9-b1ab-141587abe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select individuals from site 7\n",
    "X_test_7=X_test[batch_effects_test[:,0]==7]\n",
    "n_synthetic_samples = sum(batch_effects_test[:,0]==7)\n",
    "\n",
    "# Create a batch effect file for forward predictions in site 8. We hence select 8 as batch effect ID. \n",
    "# We choose gender 0 and site 8.\n",
    "be = np.zeros((n_synthetic_samples,2))\n",
    "eights = np.ones((n_synthetic_samples,1))*8\n",
    "be[:,0] = eights[:,0]\n",
    "be=be.astype(int)\n",
    "\n",
    "# save individuals and create path\n",
    "with open('X_test_7_in_8.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_test_7), file)\n",
    "    \n",
    "X_test_7_path = os.path.join(processing_dir, 'X_test_7_in_8.pkl')\n",
    "\n",
    "#save batch effect and create path\n",
    "with open('be_8.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(be), file)\n",
    "    \n",
    "be_8_path = os.path.join(processing_dir, 'be_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7169a1-0ede-41c4-82ed-4dc722c5411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a forward prediction for the individuals in site 7 for the batch effect of site 8.\n",
    "\n",
    "ptk.normative.predict(covfile=X_test_7_path,\n",
    "                      inputsuffix='_estimate',\n",
    "                      respfile=None,\n",
    "                      tsbefile=be_8_path,\n",
    "                      model_path=output_path,\n",
    "                      alg='hbr',\n",
    "                      outputsuffix='_X_test_7_in_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8852011-2688-4e90-bf5b-1f6bdf6aeda3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Case 2: Load the yhat and s2 for subjects from site 7 for site 8\n",
    "\n",
    "Now we load the data we have just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d28ae8-d537-4489-815f-c09924beb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_Xtest7in8 = ldpkl(os.path.join(processing_dir, \"yhat_Xtest7in8.pkl\"))\n",
    "ys2_Xtest7in8 = ldpkl(os.path.join(processing_dir, \"ys2_Xtest7in8.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9cf6f0-b37e-48ab-a6c6-5e746c33fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also load the original z-scores.\n",
    "z_scores = ldpkl(os.path.join(processing_dir, \"Z_estimate.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45afe6-3ad4-460a-8b75-3f38aaf05f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data. We only select the first IDP.\n",
    "\n",
    "yhat_Xtest7in8[0]\n",
    "ys2_Xtest7in8[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7f741-5142-438e-b4b8-71c1a5da6e26",
   "metadata": {},
   "source": [
    "### Case 2: Back calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5c7cf-e3e8-4c91-a022-bdab3e9ded20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the back transformation\n",
    "y_adj = z_scores[batch_effects_test[:,0]==7][0].to_numpy() * ys2_Xtest7in8[0].to_numpy() - yhat_Xtest7in8[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93c234-f73f-4069-b988-3a1c264efe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_adj = y_adj * (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32be6f-4b12-475a-a33d-28a7bb77d4ce",
   "metadata": {},
   "source": [
    "### Case 2: Plotting\n",
    "We now create a plot that contains the transfromed data from site 7, the original data from site 8 and the centiles from site 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f978c0-1e2a-48fb-907f-be63e4683d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed data from site 7,  the original data from site 8 and the centiles from site 8.\n",
    "\n",
    "plt.scatter(X_test[batch_effects_test[:,0]==8], Y_test[batch_effects_test[:,0]==8, 0])\n",
    "plt.scatter(X_test[batch_effects_test[:,0]==7], y_adj)\n",
    "# plot centiles\n",
    "for i, v in enumerate(zscores):\n",
    "    thickness = 1\n",
    "    linestyle = \"-\"\n",
    "    if v == 0:\n",
    "        thickness = 2\n",
    "    if abs(v) > 2:\n",
    "        linestyle = \"--\"\n",
    "    plt.plot(synthetic_X, q[i], linewidth = thickness, linestyle = linestyle, color = 'black', alpha = 0.7)\n",
    "\n",
    "plt.title('site ' + str(be[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb6a90-0f21-43d2-a2c5-cdbb83bf0b84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Case 3: Transter model: Make predictions/adjustments for unseen sites.\n",
    "\n",
    "The transfer requires an adaptation (training) and a test data set. We are going to use the held-out training data set for that.\n",
    "\n",
    "The trainsfer model works in two stages: \n",
    "1. The adaptation data set is used to refit the model with priors derived from the previous model\n",
    "2. Predictions and evalations are made for the hold-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9a0f7-3af0-4e0b-b293-db8c7a2c575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder for the transfer models and create directory.\n",
    "txfer_output_path = os.path.join(processing_dir,'Transfer')\n",
    "os.makedirs(txfer_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2a844-d6ad-4ef7-9ab4-4b7729e29800",
   "metadata": {},
   "source": [
    "### Case 3: Load the held out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609143a4-402a-4e84-90e0-6747ce9e68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the held out data.\n",
    "X_adapt = (new_sites_tr['age']/100).to_numpy(dtype=float)\n",
    "Y_adapt = new_sites_tr[idps].to_numpy(dtype=float)\n",
    "batch_effects_adapt = new_sites_tr[['sitenum','sex']].to_numpy(dtype=int)\n",
    "\n",
    "X_test_txfr = (new_sites_te['age']/100).to_numpy(dtype=float)\n",
    "Y_test_txfr = new_sites_te[idps].to_numpy(dtype=float)\n",
    "batch_effects_test_txfr = new_sites_te[['sitenum','sex']].to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585fae1-7172-495c-9eb1-e201d2cbe0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the holdout data.\n",
    "print(batch_effects_test_txfr.shape)\n",
    "print(batch_effects_adapt.shape)\n",
    "print(X_adapt.shape)\n",
    "print(Y_adapt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc13f1-22d5-4bbe-9334-79e02cec33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframes\n",
    "\n",
    "with open(os.path.join(processing_dir,'X_adaptation.pkl'), 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_adapt), file)\n",
    "with open(os.path.join(processing_dir,'Y_adaptation.pkl'), 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(Y_adapt), file) \n",
    "with open(os.path.join(processing_dir,'adbefile.pkl'), 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(batch_effects_adapt), file) \n",
    "\n",
    "# save the dataframes\n",
    "with open(os.path.join(processing_dir,'X_test_txfr.pkl'), 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_test_txfr), file)\n",
    "with open(os.path.join(processing_dir, 'Y_test_txfr.pkl'), 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(Y_test_txfr), file) \n",
    "with open(os.path.join(processing_dir, 'txbefile.pkl'), 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(batch_effects_test_txfr), file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5beba3-80fd-4b75-b9f3-b270da84c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the batch effect IDs.\n",
    "batch_effects_adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb677807-fba9-481b-80a0-fb33fd5f589d",
   "metadata": {},
   "source": [
    "### Case 3: Run the transfer model\n",
    "\n",
    "We now run the transfer. We do that in a loop, running over all (two) idps we have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffcc01-5eef-4339-ab75-4bb3427e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do the transfer in a loop as we are going to loop over IDPs\n",
    "\n",
    "for idx, measure in enumerate(idps):\n",
    "\n",
    "    # save an individual Y_adaptation set file\n",
    "    Y_adaptation_onefeature = Y_adapt[:,idx]\n",
    "    with open(os.path.join(txfer_output_path,f'{idx+1}'+'_Y_adaptation_onefeature.pkl'), 'wb') as file:\n",
    "        pickle.dump(pd.DataFrame(Y_adaptation_onefeature), file)\n",
    " \n",
    "    Y_test_txfr_onefeature = Y_test_txfr[:,idx]\n",
    "    with open(os.path.join(txfer_output_path,f'{idx+1}'+'_Y_test_txfr_onefeature.pkl'), 'wb') as file:\n",
    "        pickle.dump(pd.DataFrame(Y_test_txfr_onefeature), file)\n",
    " \n",
    "           \n",
    "    respfile_path = os.path.join(txfer_output_path,f'{idx+1}'+'_Y_adaptation_onefeature.pkl')\n",
    "    covfile_path = os.path.join(processing_dir, 'X_adaptation.pkl')\n",
    "    adbefile = os.path.join(processing_dir, 'adbefile.pkl')\n",
    "    \n",
    "    testrespfile_path = os.path.join(txfer_output_path, f'{idx+1}'+'_Y_test_txfr_onefeature.pkl')\n",
    "    testcovfile_path = os.path.join(processing_dir,'X_test_txfr.pkl')\n",
    "    txbefile = os.path.join(processing_dir, 'txbefile.pkl')\n",
    "   \n",
    "    output_path = os.path.join(processing_dir, 'Models/') \n",
    "    log_dir = os.path.join(processing_dir, 'log/')\n",
    "    \n",
    "\n",
    "    transfer_suffix = '_transfer_'+f'{idx+1}'\n",
    "    ptk.normative.transfer(covfile=covfile_path, \n",
    "                           respfile=respfile_path,\n",
    "                           trbefile=adbefile,\n",
    "                           model_type='bspline',\n",
    "                           linear_sigma = 'True',\n",
    "                           random_intercept_sigma = 'True',\n",
    "                           random_slope_sigma = 'True',\n",
    "                           linear_mu='True',\n",
    "                           random_intercept_mu='True',\n",
    "                           random_slope_mu='True',\n",
    "                           alg='hbr', \n",
    "                           model_path = output_path,\n",
    "                           log_path=log_dir, \n",
    "                           binary=True,\n",
    "                           output_path=txfer_output_path, \n",
    "                           testcov=testcovfile_path,\n",
    "                           tsbefile=txbefile,\n",
    "                           #trbefile=trbefile,\n",
    "                           testresp = testrespfile_path,\n",
    "                           inputsuffix = \"_estimate\",\n",
    "                           outputsuffix=transfer_suffix, \n",
    "                           savemodel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bcaed-6b92-42c1-aafe-349d24a051d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transfer mean and variance.\n",
    "\n",
    "yhat_transfer = ldpkl(os.path.join(processing_dir, \"yhat_transfer1.pkl\"))\n",
    "ys2_transfer = ldpkl(os.path.join(processing_dir, \"ys2_transfer1.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe8a2e-14ca-4f6a-bed1-19d521b6b288",
   "metadata": {},
   "source": [
    "Again, we create dummy data to use the get_mcmc_quantiles function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421144e-c766-47cb-b7e1-1b809394b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy zscores\n",
    "zscores = np.arange(-3,4)[:,np.newaxis]\n",
    "\n",
    "# We need to provide a matrix of shape [N,d]. In this case d=1, so we need to expand this matrix with a single 'empty' dimension.\n",
    "# We can do this by using the np.newaxis in this way.\n",
    "# Create sythetic X achsis\n",
    "n_synthetic_samples = 200\n",
    "synthetic_X = np.linspace(0.15, 0.85, n_synthetic_samples)[:,np.newaxis]\n",
    "# choose type of site effect by cerating intercept offset (or not).  We choose gender 0 and site 21.\n",
    "be = np.zeros((n_synthetic_samples,2))\n",
    "twenties = np.ones((n_synthetic_samples,1))*21\n",
    "be[:,0] = twenties[:,0]\n",
    "be=be.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb808c06-99e0-475f-9e63-09a4330a7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transfer model. Make sure to choose the transfer path.\n",
    "nm_transfer1 = ldpkl(os.path.join(txfer_output_path, 'NM_0_0_transfer1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986a111-1b52-44d1-85c4-a54478487bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate an object to hold the quantiles\n",
    "q = nm_transfer1.get_mcmc_quantiles(synthetic_X, be,  zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab681fa9-913e-4a6e-8a46-46b4d55ff300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data onto the centiles obtained from the transfer model\n",
    "plt.scatter(X_test_txfr[batch_effects_test_txfr[:,0]==21], Y_test_txfr[(batch_effects_test_txfr[:,0]==21),0])\n",
    "# Note: we are again plotting both genders. In a real analysis example, one might want to create two plots, one for each gender.\n",
    "for i, v in enumerate(zscores):\n",
    "    thickness = 1\n",
    "    linestyle = \"-\"\n",
    "    if v == 0:\n",
    "        thickness = 2\n",
    "    if abs(v) > 2:\n",
    "        linestyle = \"--\"\n",
    "    plt.plot(synthetic_X, q[i], linewidth = thickness, linestyle = linestyle, color = 'black', alpha = 0.7)\n",
    "\n",
    "plt.title('site ' + str(be[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9f9b0-f105-46ad-8059-383154fbd6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
