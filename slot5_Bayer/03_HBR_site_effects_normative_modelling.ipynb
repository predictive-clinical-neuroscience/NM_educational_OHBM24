{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d09a59a-310b-4565-95f5-1f3e9721fdec",
   "metadata": {},
   "source": [
    "# Normative modelling as site effect correction tool\n",
    "\n",
    "This tutorial will walk you through the application of normative modelling as site effect correction tool.\n",
    "\n",
    "You will learn:\n",
    "- How to fit a normative model to data from different sites\n",
    "- choices to make about the model\n",
    "- how to transfer data from one site into another\n",
    "- how to make predictions for unseen sites and the rationale behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaac9b3-3ad6-47ee-a06f-528f7a0c0aae",
   "metadata": {},
   "source": [
    "# Quick recap: normative models using the pcn toolkit\n",
    "A normative model requires three input variables:\n",
    "- covariates (the X axis, ususally age)\n",
    "- response variables (usually one or several brain feature measurements)\n",
    "- a batch effect file. Batch effects can be anything that interferes with the covrariate (sex, site)\n",
    "\n",
    "While the blr model reuiqres input of the batch effects via a design matrix, the implementation of the hbr only reuires a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5248840-9b00-4820-8593-85d9d7ad4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pcntoolkit\n",
    "import os\n",
    "import pandas as pd\n",
    "import pcntoolkit as ptk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c07f21-42ea-4645-b701-ef9c028098c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103186c8-97aa-49e4-a6d1-72daac14d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create working dir\n",
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f2874-4006-4703-ad49-17ff3f0c20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae41f1b-5a89-4e6a-b65f-38f09510cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_folder = \"Site_effect_tutorial/\"\n",
    "os.chdir(processing_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33d09a-63f4-48b0-b609-ecdbbccba52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(processing_folder):\n",
    "    os.makedirs(processing_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13046ee6-7bbd-43c0-9ed4-4f4d945b8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(tutdir)\n",
    "os.chdir(processing_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77debe6f-3a1f-4f8f-9d99-99a6dba6718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir = os.getcwd()\n",
    "print(f\"The processing directory is: {processing_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639454a-4bc9-4b23-bed1-7baadfb9f3b5",
   "metadata": {},
   "source": [
    "# 01. Data pre-processing.\n",
    "\n",
    "### Load the data\n",
    "\n",
    "For this tutorial, we use publicly availabe data that has been pooled from various data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08aa693-1a88-416e-8012-ce3d1ca008dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "!wget -nc https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_tr.csv\n",
    "!wget -nc https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_te.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3837b-b424-48a2-8416-1505f50c845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data into the notebook\n",
    "test_data = os.path.join(root_dir, 'fcon1000_te.csv')\n",
    "df_te = pd.read_csv(test_data, index_col=0)\n",
    "\n",
    "# remove some bad subjects\n",
    "# df_te, bad_sub = remove_bad_subjects(df_te, df_te)\n",
    "\n",
    "# extract a list of unique site ids from the test set\n",
    "site_ids_te =  sorted(set(df_te['site'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a2cc6-721d-4a41-b5c7-6577715ab522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data into the notebook\n",
    "train_data = os.path.join(root_dir, 'fcon1000_tr.csv')\n",
    "\n",
    "df_tr = pd.read_csv(train_data, index_col=0)\n",
    "\n",
    "# remove some bad subjects\n",
    "#df_ad, bad_sub = remove_bad_subjects(df_ad, df_ad)\n",
    "\n",
    "# extract a list of unique site ids from the test set\n",
    "site_ids_ad =  sorted(set(df_tr['site'].to_list()))\n",
    "\n",
    "if not all(elem in site_ids_ad for elem in site_ids_te):\n",
    "    print('Warning: some of the testing sites are not in the adaptation data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0374f4-58a1-41ee-b8fc-9a8202152b5c",
   "metadata": {},
   "source": [
    "### Make a quick display of the age distibution per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37856974-a5c2-4e9c-abd2-aab2338071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = df_tr.hist(['age'], by='site', layout=(5,5),\n",
    "                  legend=True, yrot=90,sharex=True,sharey=True, \n",
    "                  figsize=(6,6))\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlabel('age')\n",
    "    ax.set_ylabel('distribution')\n",
    "    ax.set_ylim(bottom=1,top=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5085f-977d-4739-ab27-c8856da0037e",
   "metadata": {},
   "source": [
    "### Additionally, print the number of subjects per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08ae6f-90c2-4bba-9d81-4c46c1486be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df_tr['site'].unique()\n",
    "\n",
    "print('sample size check')\n",
    "for i,s in enumerate(sites):\n",
    "    idx = df_tr['site'] == s\n",
    "    idxte = df_te['site'] == s\n",
    "    print(i,s, sum(idx), sum(idxte))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d410c87-a944-4294-bada-39db20a6a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['sitenum'] = 0\n",
    "for i,s in enumerate(sites):\n",
    "    idx = df_tr['site'] == s\n",
    "    df_tr['sitenum'].loc[idx] = i\n",
    "\n",
    "    print('site',s, sum(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d05f5c-a9c0-427a-a673-88e7f1d1435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te['sitenum'] = 0\n",
    "for i,s in enumerate(sites):\n",
    "    idx = df_te['site'] == s\n",
    "    df_te['sitenum'].loc[idx] = i\n",
    "\n",
    "    print('site',s, sum(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e32cf1-b2c0-4fea-beec-d3034d933086",
   "metadata": {},
   "source": [
    "### Set aside hold-out transfer sites.\n",
    "For our site transfer later we want to exclude some sites from the training and test set for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a0979-6606-47b4-aebd-c6e16516f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some sites for later, these are going to be my new sites\n",
    "new_sites_tr = df_tr[(df_tr['site']== 'SaintLouis')| (df_tr['site']=='COBRE')]\n",
    "new_siets_te = df_te[(df_te['site']== 'SaintLouis')| (df_te['site']=='COBRE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a4ccb-5d2e-40c4-8c84-8c1c926bb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove those sites from the training and test sets\n",
    "df_tr = df_tr[df_tr.site != 'SaintLouis']\n",
    "df_tr = df_tr[df_tr.site != 'COBRE']\n",
    "df_te = df_te[df_te.site != 'SaintLouis']\n",
    "df_te = df_te[df_te.site != 'COBRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195228da-3046-411d-872b-a0f6e2b2fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the IDPs (columns) of interest from the data frame\n",
    "idps = ['lh_G&S_frontomargin_thickness','rh_G&S_frontomargin_thickness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a59ef4-3a45-4467-88ba-c8cd87e034c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data frames for training and testing\n",
    "\n",
    "X_train = (df_tr['age']/100).to_numpy(dtype=float)\n",
    "Y_train = df_tr[idps].to_numpy(dtype=float)\n",
    "batch_effects_train = df_tr[['sitenum', 'sex']].to_numpy(dtype=int)\n",
    "\n",
    "#test_data[['site_id','sex']].to_numpy(dtype=float)\n",
    "\n",
    "# save data\n",
    "with open('X_train.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_train), file)\n",
    "with open('Y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(Y_train), file)\n",
    "with open('trbefile.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(batch_effects_train), file)\n",
    "\n",
    "\n",
    "X_test = (df_te['age']/100).to_numpy(dtype=float)\n",
    "Y_test = df_te[idps].to_numpy(dtype=float)\n",
    "batch_effects_test = df_te[['sitenum', 'sex']].to_numpy(dtype=int)\n",
    "\n",
    "#save data\n",
    "with open('X_test.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_test), file)\n",
    "with open('Y_test.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(Y_test), file)\n",
    "with open('tsbefile.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(batch_effects_test), file)\n",
    "\n",
    "# a simple function to quickly load pickle files\n",
    "def ldpkl(filename: str):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a773bc7-3974-4c1d-a914-84ed53be9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple function to quickly load pickle files\n",
    "def ldpkl(filename: str):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafbcac-9518-4a0b-8efb-85c8be440328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model needs the paths to the data. create them here\n",
    "\n",
    "respfile = os.path.join(processing_dir, 'Y_train.pkl')       # measurements  (eg cortical thickness) of the training samples (columns: the various features/ROIs, rows: observations or subjects)\n",
    "covfile = os.path.join(processing_dir, 'X_train.pkl')        # covariates (eg age) the training samples (columns: covariates, rows: observations or subjects)\n",
    "\n",
    "testrespfile_path = os.path.join(processing_dir, 'Y_test.pkl')       # measurements  for the testing samples\n",
    "testcovfile_path = os.path.join(processing_dir, 'X_test.pkl')        # covariate file for the testing samples\n",
    "\n",
    "trbefile = os.path.join(processing_dir, 'trbefile.pkl')      # training batch effects file (eg scanner_id, gender)  (columns: the various batch effects, rows: observations or subjects)\n",
    "tsbefile = os.path.join(processing_dir, 'tsbefile.pkl')      # testing batch effects file\n",
    "\n",
    "output_path = os.path.join(processing_dir, 'Models/')    #  output path, where the models will be written\n",
    "log_dir = os.path.join(processing_dir, 'log/')           # log path\n",
    "if not os.path.isdir(output_path):\n",
    "    os.mkdir(output_path)\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "outputsuffix = 'estimate'      # a string to name the output files, of use only to you, so adapt it for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff7978-0dbf-41b8-8b83-fa17eed41e55",
   "metadata": {},
   "source": [
    "# 02. Normative modelling\n",
    "## Case 1: Training a nomrative model on different sites and make predictions on the same sites (equal distribution of sites in training and test data)\n",
    "\n",
    "The full model contains a training set (covariates, responses, batch effects) and test set (covariates, responses, batch effects). All batch effects are distributes equally across training and test set.\n",
    "\n",
    "Reqires:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff2cc7-8102-4157-be8f-b915da827955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "ptk.normative.estimate(covfile=covfile, \n",
    "                       respfile=respfile,\n",
    "                       tsbefile=tsbefile, \n",
    "                       model_type='bspline',\n",
    "                       linear_sigma = 'True',\n",
    "                       random_intercept_sigma = 'True',\n",
    "                       random_slope_sigma = 'True',\n",
    "                       linear_mu='True',\n",
    "                       random_intercept_mu='True',\n",
    "                       random_slope_mu='True',\n",
    "                       alg='hbr', \n",
    "                       log_path=log_dir, \n",
    "                       binary=True,\n",
    "                       output_path=output_path, \n",
    "                       testcov= testcovfile_path,\n",
    "                       testresp = testrespfile_path,\n",
    "                       trbefile=trbefile,\n",
    "                       outputsuffix=outputsuffix, \n",
    "                       savemodel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e6828-de3e-4f9b-b9ee-254fcf9e3d15",
   "metadata": {},
   "source": [
    "# Case 1: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94234b-fc23-4c6c-89f0-add4a1f54cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the estimated model for the first IDP (column)\n",
    "model1_estimate = ldpkl(os.path.join(processing_dir, \"Models/NM_0_0_estimate.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00434b7-b388-4019-aa4f-1dd7f0fb2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can always have a display of all available methods using the help call:\n",
    "help(model1_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995b7e7-4a8d-43d2-a81c-7cab230b8c0e",
   "metadata": {},
   "source": [
    "## Case 1a: plot the results for one site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7585f5-200e-4481-94dd-d793b78bd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy zscores\n",
    "zscores = np.arange(-3,4)[:,np.newaxis]\n",
    "\n",
    "# We need to provide a matrix of shape [N,d]. In this case d=1, so we need to expand this matrix with a single 'empty' dimension.\n",
    "# We can do this by using the np.newaxis in this way.\n",
    "# Create sythetic X achsis\n",
    "n_synthetic_samples = 200\n",
    "synthetic_X = np.linspace(0.15, 0.85, n_synthetic_samples)[:,np.newaxis]\n",
    "# choose type of site effect by cerating intercept offset (or not)\n",
    "be = np.zeros((n_synthetic_samples,2))\n",
    "sevens = np.ones((n_synthetic_samples,1))*8\n",
    "be[:,0] = sevens[:,0]\n",
    "be=be.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fdf51d-7074-49c6-8aa2-f44a585e0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantiles based on x achsis, batch effetcs be and zscore range\n",
    "q = model1_estimate.get_mcmc_quantiles(synthetic_X, be, zscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086050b-27e1-49d6-9385-a3ada654be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original training set data points (site 0) into the centiles\n",
    "plt.scatter(X_test[batch_effects_test[:,0]==8], Y_test[batch_effects_test[:,0]==8, 0])\n",
    "for i, v in enumerate(zscores):\n",
    "    thickness = 1\n",
    "    linestyle = \"-\"\n",
    "    if v == 0:\n",
    "        thickness = 2\n",
    "    if abs(v) > 2:\n",
    "        linestyle = \"--\"\n",
    "    plt.plot(synthetic_X, q[i], linewidth = thickness, linestyle = linestyle, color = 'black', alpha = 0.7)\n",
    "\n",
    "plt.title('site ' + str(be[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496923cb-72d2-4fa9-8122-d02693e91b05",
   "metadata": {},
   "source": [
    "## Case 1b: Add data points from site 7 into the quantile plot of site 8.\n",
    "\n",
    "1. Create a forward prediction for the covariate combination of each individual of site 7 for site 8\n",
    "2. Use yhat and s2 to invert z score of those individuals\n",
    "\n",
    "Rationale:\n",
    "\n",
    "The zscores that the model produces are normalized for site. Hence, the z-socres are comparable in z-score space.\n",
    "In order to convert the data points back into different site spaces, we need to revert the z-score transformation.\n",
    "\n",
    "$z_7 = z_8$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a893a6-4b23-4cf9-b1ab-141587abe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select individuals from site 7\n",
    "\n",
    "X_test_7=X_test[batch_effects_test[:,0]==7]\n",
    "n_synthetic_samples = sum(batch_effects_test[:,0]==7)\n",
    "\n",
    "# Create a batch effect file for forward predictions in site\n",
    "be = np.zeros((n_synthetic_samples,2))\n",
    "eights = np.ones((n_synthetic_samples,1))*8\n",
    "be[:,0] = eights[:,0]\n",
    "be=be.astype(int)\n",
    "\n",
    "with open('X_test_7.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(X_test_7), file)\n",
    "\n",
    "X_test_7_path = os.path.join(processing_dir, 'X_test_7.pkl')\n",
    "\n",
    "with open('be_8.pkl', 'wb') as file:\n",
    "    pickle.dump(pd.DataFrame(be), file)\n",
    "\n",
    "be_8_path = os.path.join(processing_dir, 'be_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1159826-a6fa-4309-a424-a584efa00d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(batch_effects_test[:,0]==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fd2e6-ff1a-4937-b9d6-a084f026a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptk.normative.predict(covfile=X_test_7_path,\n",
    "                      inputsuffix='_estimate',\n",
    "                      respfile=None,\n",
    "                      tsbefile=be_8_path,\n",
    "                      model_path=output_path,\n",
    "                      alg='hbr',\n",
    "                      outputsuffix='_X_test_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8852011-2688-4e90-bf5b-1f6bdf6aeda3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### load the yhat and s2 for subjects from site 7 for site 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d28ae8-d537-4489-815f-c09924beb68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = ldpkl(os.path.join(processing_dir, \"yhat_Xtest7.pkl\"))\n",
    "ys2 = ldpkl(os.path.join(processing_dir, \"ys2_Xtest7.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62643a8-7d5b-45e2-8a9e-e2b58a7ad1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original test set data points (site 1) into the percentiles\n",
    "\n",
    "plt.scatter(X_test[150:214], Y_test[150:214, 1])\n",
    "for i, v in enumerate(zscores):\n",
    "    thickness = 1\n",
    "    linestyle = \"-\"\n",
    "    if v == 0:\n",
    "        thickness = 2\n",
    "    if abs(v) > 2:\n",
    "        linestyle = \"--\"\n",
    "    plt.plot(synthetic_X, q[i], linewidth = thickness, linestyle = linestyle, color = 'black', alpha = 0.7)\n",
    "\n",
    "plt.title('site ' + str(be[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6beaf-2348-4b68-b57c-68bc46819c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to above, but for the second site (np.ones)\n",
    "zscores = np.arange(-4,4)[:,np.newaxis]\n",
    "\n",
    "# We need to provide a matrix of shape [N,d]. In this case d=1, so we need to expand this matrix with a single 'empty' dimension.\n",
    "# We can do this by using the np.newaxis in this way.\n",
    "n_synthetic_samples = 200\n",
    "synthetic_X = np.linspace(0.15, 0.85, n_synthetic_samples)[:,np.newaxis]\n",
    "be = np.ones((n_synthetic_samples,1))*2\n",
    "be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95b83c-078f-4238-b856-69ba87dba148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scoers from the model\n",
    "\n",
    "yhat = ldpkl(os.path.join(processing_dir, \"yhat_estimate.pkl\"))\n",
    "z_scores = ldpkl(os.path.join(processing_dir, \"Z_estimate.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9b4d7-a8c9-4bab-8923-92fd62dd371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb6a90-0f21-43d2-a2c5-cdbb83bf0b84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffcc01-5eef-4339-ab75-4bb3427e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsuffix= \"_fit\"\n",
    "ptk.normative.fit(covfile=covfile, \n",
    "                       respfile=respfile,\n",
    "                       tsbefile=tsbefile, \n",
    "                       model_type='bspline',\n",
    "                       linear_sigma = 'True',\n",
    "                       random_intercept_sigma = 'True',\n",
    "                       random_slope_sigma = 'True',\n",
    "                       linear_mu='True',\n",
    "                       random_intercept_mu='True',\n",
    "                       random_slope_mu='True',\n",
    "                       alg='hbr', \n",
    "                       log_path=log_dir, \n",
    "                       binary=True,\n",
    "                       output_path=output_path, \n",
    "                       #testcov=testcovfile_path,\n",
    "                       #trbefile=trbefile,\n",
    "                       #trbefile=be_path,\n",
    "                       #testcov=\"covariate_forwardmodel.txt\",\n",
    "                       #testcov=synthetic_path,\n",
    "                       #testresp = testrespfile_path,\n",
    "                       outputsuffix=outputsuffix, \n",
    "                       savemodel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bcaed-6b92-42c1-aafe-349d24a051d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_forward = ldpkl(os.path.join(processing_dir, \"yhat_forward.pkl\"))\n",
    "yhat_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184283c-5678-44aa-8f59-8b2be638e8ba",
   "metadata": {},
   "source": [
    "# Case 2: Transfer to new sites\n",
    "\n",
    "The second case we are going to look at in this tutorial is to use a trained model to make a transfer to new, unseen sites. \n",
    "The \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d626c71-b23d-48c7-ab4a-d034e4ac2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptk.normative.transfer(covfile=testcovfile_path,\n",
    "                      #covfile=synthetic_path,\n",
    "                      inputsuffix='_estimate',\n",
    "                      respfile=None,\n",
    "                      tsbefile=tsbefile,\n",
    "                      trbefile=trbefile,\n",
    "                      model_path=output_path,\n",
    "                      alg='hbr',\n",
    "                      outputsuffix='_transfer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421144e-c766-47cb-b7e1-1b809394b63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
