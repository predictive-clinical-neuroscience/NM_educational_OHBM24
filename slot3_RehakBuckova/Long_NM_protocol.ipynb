{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Predictive Clinical Neuroscience Toolkit](https://github.com/amarquand/PCNtoolkit) \n",
    "# The Normative Modeling Framework for Computational Psychiatry Protocol\n",
    "## Using Pre-trained Normative Models for Longitudinal Cortical Thickness Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by [Barbora Rehák Bučková](https://twitter.com/BarboraRehak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Protocol_overview.png\" alt=\"Pipeline overview\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Install necessary libraries & grab data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.\n",
    "Begin by creating the conda virtual environment (if you're working on your own computer). You can follow the recommendation at the PCN toolkit [webpage](https://github.com/amarquand/PCNtoolkit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda create -y python==3.9 --name=PCN\n",
    "! conda activate PCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the GitHub repository, it contains the necessary code and example data. Additionally, clone the repository with pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, install the PCN toolkit\n",
    "! pip install pcntoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also clone the pretrained models repository and unpack the relevant models\n",
    "! git clone https://github.com/predictive-clinical-neuroscience/braincharts.git\n",
    "\n",
    "import os\n",
    "os.chdir('/content/braincharts/models')\n",
    "\n",
    "! unzip lifespan_57K_82sites.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now install the rest of the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on where you are locally\n",
    "#! pip install -r requirements.txt\n",
    "\n",
    "# Google Colab\n",
    "! pip install -r /content/PCNtoolkit-demo/tutorials/Long_NM/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case something goes wrong, try to install the packages individually (especially `nilearn` is not included in the pcntoolkit installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nilearn # Only needed for surface plotting\n",
    "import xarray as xr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pcntoolkit.util.utils import create_design_matrix, compute_MSLL\n",
    "from pcntoolkit.normative import estimate, predict, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this path to the git cloned PCNtoolkit-demo repository --> Uncomment whichever line you need for either running on your own computer or on Google Colab.\n",
    "# working_dir = '<path to you>/PCNtoolkit-demo/tutorials/Long_NM'\n",
    "# pretrained_dir = '<path to you>/braincharts'\n",
    "# target_dir = '<path to you>/Results'\n",
    "\n",
    "# Google Colab alternative\n",
    "working_dir = '/content/PCNtoolkit-demo/tutorials/Long_NM'\n",
    "pretrained_dir = '/content/braincharts'\n",
    "target_dir = '/content/Results'\n",
    "os.makedirs(target_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "v1_pat = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_patients_v1.csv'), index_col=0)\n",
    "v2_pat = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_patients_v2.csv'), index_col=0)\n",
    "v1_con = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_controls_v1.csv'), index_col=0)\n",
    "v2_con = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_controls_v2.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.\n",
    "### Run normative models\n",
    "\n",
    "Initially, this tutorial follows the [BLR_normativemodel_protocol](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/tree/main/tutorials/BLR_protocol)\n",
    "\n",
    "- the healthy control groups has to be split into two parts\n",
    "    - the first part ensures the pre-trained model is well-adjusted for your scanning site\n",
    "    - the second part is used to compute the healthy variation in controls\n",
    "\n",
    "Alternatively, if you don't have enough controls, you can  drop the site-adjustment step, however, **you then can not interpret the cross-sectional results!**\n",
    "\n",
    "- for more details on this, see our [paper](https://www.biorxiv.org/content/10.1101/2023.06.09.544217v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the control group \n",
    "test_size = 0.3\n",
    "index_controls_adjust, index_controls_test = train_test_split(v1_con.index, test_size = test_size, shuffle = True, random_state = 42, stratify=v1_con['sex'])\n",
    "index_patients_test = list(v1_pat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup details regarding covariates and the spline fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which data columns are covariates?\n",
    "covariates = ['age', 'sex']\n",
    "\n",
    "# Limits for cubic B-spline basis\n",
    "xmin = -5\n",
    "xmax = 100\n",
    "\n",
    "# Absolute Z threshold above which a sample is considered to be an outlier (without fitting any model)\n",
    "outlier_thresh = 7\n",
    "\n",
    "# load the site IDs from the pretrained data (if you decide to change the pretrained models, don;t forget to also change site ids here)\n",
    "with open(os.path.join(pretrained_dir, 'docs', 'site_ids_ct_82sites.txt')) as f:\n",
    "    site_ids_pretrained = f.read().splitlines()\n",
    "\n",
    "# load names from the Destrioux freesurfer parcellation atlas\n",
    "with open(os.path.join(pretrained_dir,'docs','phenotypes_ct_lh.txt')) as f:\n",
    "    idp_ids_lh = f.read().splitlines()\n",
    "with open(os.path.join(pretrained_dir,'docs','phenotypes_ct_rh.txt')) as f:\n",
    "    idp_ids_rh = f.read().splitlines()\n",
    "\n",
    "idp_ids = idp_ids_lh + idp_ids_rh\n",
    "idp_ids.remove('lh_MeanThickness_thickness')\n",
    "idp_ids.remove('rh_MeanThickness_thickness')\n",
    "\n",
    "# Which sites are present in our dataset?\n",
    "site_ids_test = pd.concat([v1_pat, v1_con], axis=0)['site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, we could concatenate all the visits and groups (apart from controls for adjustment) and run the model. However, for educational purposes, we will run it separately for each class and each visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model separately for every visit\n",
    "for ivisit in range(1,3):\n",
    "    \n",
    "    # So that we know, what's going on\n",
    "    print( 'Running models for visit no: '+str(ivisit))\n",
    "\n",
    "    visit_dir = os.path.join(target_dir, 'V'+str(ivisit))\n",
    "    os.makedirs(visit_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Which data is going to be analysed in which cycle\n",
    "    if ivisit == 1:    \n",
    "        df_controls_adjust = v1_con.loc[index_controls_adjust]\n",
    "        df_controls_test = v1_con.loc[index_controls_test]\n",
    "        df_patients_test = v1_pat\n",
    "\n",
    "    elif ivisit == 2:\n",
    "        # Note that we are using the first visit of controls as site adjustment in both visits. This indeed means that the models are the same and that it could be tchnically run for all visits and groups together\n",
    "        df_controls_adjust = v1_con.loc[index_controls_adjust]\n",
    "        df_controls_test = v2_con.loc[index_controls_test]\n",
    "        df_patients_test = v2_pat\n",
    "\n",
    "    # For sanity checks, save the files into *.csv\n",
    "    df_controls_adjust.to_csv(os.path.join(visit_dir, 'v'+str(ivisit)+'_controls_adjust.csv'), sep=' ')\n",
    "    df_controls_test.to_csv(os.path.join(visit_dir, 'v'+str(ivisit)+'_controls_test.csv'), sep=' ')\n",
    "    df_patients_test.to_csv(os.path.join(visit_dir, 'v'+str(ivisit)+'_patients_test.csv'), sep=' ')\n",
    "\n",
    "    ###\n",
    "    # The pretrained model coeffitients are stored individually for every IDP, so we need to cycle through them and apply the models\n",
    "    ###\n",
    "    for iidp_no, iidp in enumerate(idp_ids):\n",
    "        # Create folder for the particular IDP (in appropriate visit dir)\n",
    "        idp_visit_dir = os.path.join(visit_dir, iidp)\n",
    "        os.makedirs(idp_visit_dir, exist_ok=True)\n",
    "        os.chdir(idp_visit_dir)\n",
    "        # Assuming that the data used were not a part of the original dataset on which the model was trained\n",
    "        if not all (elem in site_ids_pretrained for elem in site_ids_test):   \n",
    "            \n",
    "            print('The data are not part of the original dataset')\n",
    "            \n",
    "            # Configure and save the design matrices    \n",
    "            # Adaptation control data\n",
    "            design_matrix_controls_adjust = create_design_matrix(df_controls_adjust[covariates],\n",
    "                                                    site_ids = df_controls_adjust['site'],\n",
    "                                                    all_sites = site_ids_pretrained,\n",
    "                                                    basis = 'bspline', xmin = xmin, xmax = xmax)\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'design_matrix_controls_adjust.txt'), design_matrix_controls_adjust)\n",
    "\n",
    "            # The rest of controls\n",
    "            design_matrix_controls_test = create_design_matrix(df_controls_test[covariates],\n",
    "                                                    site_ids = df_controls_test['site'],\n",
    "                                                    all_sites = site_ids_pretrained,\n",
    "                                                    basis = 'bspline', xmin = xmin, xmax = xmax)\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'design_matrix_controls_test.txt'), design_matrix_controls_test)\n",
    "\n",
    "            # Patients\n",
    "            design_matrix_patients_test = create_design_matrix(df_patients_test[covariates],\n",
    "                                                    site_ids = df_patients_test['site'],\n",
    "                                                    all_sites = site_ids_pretrained,\n",
    "                                                    basis = 'bspline', xmin = xmin, xmax = xmax)\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'design_matrix_patients_test.txt'), design_matrix_patients_test)\n",
    "\n",
    "            ###\n",
    "            # Save responses\n",
    "            ###\n",
    "            # Adatation control data\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'response_controls_adjust.txt'), df_controls_adjust[iidp].to_numpy())\n",
    "            # Test control data\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'response_controls_test.txt'), df_controls_test[iidp].to_numpy())\n",
    "            # Test patient data\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'response_patients_test.txt'), df_patients_test[iidp].to_numpy())\n",
    "\n",
    "            ###\n",
    "            # Save sitenum\n",
    "            ###\n",
    "            # Adatation control data\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'sitenum_controls_adjust.txt'), df_controls_adjust['sitenum'].to_numpy())\n",
    "            # Test control data\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'sitenum_controls_test.txt'), df_controls_test['sitenum'].to_numpy())\n",
    "            # Test patient data\n",
    "            np.savetxt(os.path.join(idp_visit_dir, 'sitenum_patients_test.txt'), df_patients_test['sitenum'].to_numpy())\n",
    "\n",
    "            ###\n",
    "            # Actually run the model\n",
    "            ###\n",
    "            yhat, s2, Z, y = predict(os.path.join(idp_visit_dir, 'design_matrix_controls_test.txt'),\n",
    "                                     respfile = os.path.join(idp_visit_dir, 'response_controls_test.txt'),\n",
    "                                     alg='blr',\n",
    "                                     model_path = os.path.join(pretrained_dir, 'models', 'lifespan_57K_82sites', iidp, 'Models'),\n",
    "                                     adaptrespfile = os.path.join(idp_visit_dir, 'response_controls_adjust.txt'),\n",
    "                                     adaptcovfile = os.path.join(idp_visit_dir, 'design_matrix_controls_adjust.txt'),\n",
    "                                     adaptvargroupfile = os.path.join(idp_visit_dir, 'sitenum_controls_adjust.txt'),\n",
    "                                     testvargroupfile = os.path.join(idp_visit_dir, 'sitenum_controls_test.txt'),\n",
    "                                     outputsuffix = 'controls_test',\n",
    "                                     return_y = True\n",
    "                                     )\n",
    "            \n",
    "            yhat, s2, Z, y = predict(os.path.join(idp_visit_dir, 'design_matrix_patients_test.txt'),\n",
    "                                     respfile = os.path.join(idp_visit_dir, 'response_patients_test.txt'),\n",
    "                                     alg='blr',\n",
    "                                     model_path = os.path.join(pretrained_dir, 'models', 'lifespan_57K_82sites', iidp, 'Models'),\n",
    "                                     adaptrespfile = os.path.join(idp_visit_dir, 'response_controls_adjust.txt'),\n",
    "                                     adaptcovfile = os.path.join(idp_visit_dir, 'design_matrix_controls_adjust.txt'),\n",
    "                                     adaptvargroupfile = os.path.join(idp_visit_dir, 'sitenum_controls_adjust.txt'),\n",
    "                                     testvargroupfile = os.path.join(idp_visit_dir, 'sitenum_patients_test.txt'),\n",
    "                                     outputsuffix = 'patients_test',\n",
    "                                     return_y = True\n",
    "                                     )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, we have a sizeable folder structure, that's hard to navigate and is not necessary, as we only splitted data once. Apply the following functions in order to concatenate the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idp_concat_quality(target_dir, idp_ids, suffix='predict'):\n",
    "    \"\"\"\n",
    "    Concatenate the quality measures over all idps\n",
    "    returns pandas table of \n",
    "    \"\"\"\n",
    "    quality_measures = ['EXPV', 'Rho', 'pRho', 'RMSE', 'SMSE']\n",
    "\n",
    "    qm = np.empty([len(idp_ids), len(quality_measures)])\n",
    "    for i, idp in enumerate(idp_ids):\n",
    "        for j, iq in enumerate(quality_measures):\n",
    "            qm[i,j] = np.genfromtxt(os.path.join(target_dir, idp, iq+'_'+suffix +'.txt' ), delimiter=' ')\n",
    "\n",
    "    qm = pd.DataFrame(qm, columns=quality_measures, index=idp_ids)\n",
    "\n",
    "    return(qm)\n",
    "\n",
    "def idp_concat(m_dir, f_name, idp_ids, t_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Concatenates a vector file across all IDP's and writes\n",
    "    returns the path to the file\n",
    "    \n",
    "    file_path = idp_concat(m_dir, f_name, idp_ids, t_name, **kwargs)\n",
    "        - m_dir = main dir with all the models\n",
    "        - f_name = the textfile to load across all models\n",
    "        - idp_ids = list of idps\n",
    "        - t_name = target name of the file\n",
    "        - t_dir = target directory if different than the main directory (where all the dirs for idps are)\n",
    "\n",
    "    \"\"\"\n",
    "    t_dir = kwargs.get('t_dir', m_dir)\n",
    "\n",
    "    # get dimensions of an empty array\n",
    "    l = np.genfromtxt(os.path.join(m_dir,idp_ids[0],f_name),delimiter=' ').shape[0]\n",
    "    na = np.empty([l,len(idp_ids)])\n",
    "\n",
    "    for n_idp, idp in enumerate(idp_ids):\n",
    "        na[:,n_idp] = np.genfromtxt(os.path.join(m_dir,idp,f_name),delimiter=' ')\n",
    "    \n",
    "    df_na = pd.DataFrame(na, columns = idp_ids)\n",
    "    df_na.to_csv(os.path.join(t_dir, t_name), sep = ' ', header=True, index = True)\n",
    "    \n",
    "    return(os.path.join(t_dir, t_name))\n",
    "\n",
    "def prepare_destrieux_plotting(data, hemi, method='counts'):\n",
    "    \"\"\"\n",
    "    Prepare data for ROI plotting using destrieux atlas\n",
    "    data = has to be pd.DataFrame with exactly one column\n",
    "    hemi = hemisphere 'r' or 'l'\n",
    "    method = counts/correlations/pvals \n",
    "            - this is only relevant if there is an ROI missing, it will automatically fill the missing ROI with unsignificant value (0 for counts, 1 for p-values)\n",
    "            - counts/correlations = 0\n",
    "            - pvals = 1\n",
    "    returns: return(data_mapping, view, fs_plot, fs_sulc)\n",
    "\n",
    "    Use: view = plotting.view_surf(fs_plot, data_mapping, threshold=None, symmetric_cmap=True, cmap='jet', bg_map=fs_sulc)\n",
    "    \"\"\"\n",
    "    # packages\n",
    "    from nilearn import datasets\n",
    "    import nilearn.plotting as plotting\n",
    "\n",
    "    # load destrieux atlas\n",
    "    destrieux_atlas = datasets.fetch_atlas_surf_destrieux()\n",
    "    fsaverage = datasets.fetch_surf_fsaverage()\n",
    "\n",
    "    # pick hemi\n",
    "    if hemi == 'r':\n",
    "        atlas = destrieux_atlas['map_right']\n",
    "        filter_hemi = 'rh'\n",
    "        fs_plot = fsaverage.infl_right\n",
    "        fs_sulc = fsaverage.sulc_right\n",
    "    elif hemi == 'l':\n",
    "        atlas = destrieux_atlas['map_left']\n",
    "        filter_hemi = 'lh'\n",
    "        fs_plot = fsaverage.infl_left\n",
    "        fs_sulc = fsaverage.sulc_left\n",
    "    \n",
    "    # check wthther the values are on rows or in columns\n",
    "    if [True for i in data.columns if 'occipital' in i]:\n",
    "        data = data.transpose()\n",
    "    \n",
    "    # filter the hemisphere we need (if this does not work we assume we are only given one hemisphere)\n",
    "    data_hemi = data.loc[[i for i in data.index if filter_hemi in i]]\n",
    "    if data_hemi.shape[0] == 0:\n",
    "        data_hemi = data\n",
    "    \n",
    "\n",
    "    # check whether we are missing some ROIs\n",
    "    if data_hemi.shape[0] == len(destrieux_atlas['labels']):\n",
    "        # do nothing, we have all the ROIs\n",
    "        print('all ROIs present')\n",
    "    else: \n",
    "        # run over all all labels and see whether the name exists    \n",
    "        str(destrieux_atlas['labels'][0]).split('\\'')[1]\n",
    "        missing=np.zeros(len(destrieux_atlas['labels']))\n",
    "        position=np.zeros(len(destrieux_atlas['labels']))\n",
    "        for i, imotif in enumerate(destrieux_atlas['labels']): \n",
    "            jmotif = str(imotif).split('\\'')[1]\n",
    "            \n",
    "            # change _and_ for &\n",
    "            if '_and_' in jmotif:\n",
    "                jmotif = jmotif.replace('_and_','&')\n",
    "            \n",
    "            index = []\n",
    "            index = [j for j,i in enumerate(data_hemi.index) if jmotif in i]\n",
    "            if len(index) == 0:\n",
    "                missing[i] = 1\n",
    "                position[i] = np.nan\n",
    "            else:\n",
    "                position[i] = index[0]\n",
    "\n",
    "    # finally, put together the transformed list\n",
    "    a_list = list(range(len(destrieux_atlas['labels'])))\n",
    "    data_mapping = atlas\n",
    "    for atlas_roi in a_list:\n",
    "        if np.isnan(position[atlas_roi]): # if we don't have this roi in atlas then fill with pre-defined value\n",
    "            if method == 'counts' or method == 'correlations':\n",
    "                data_mapping = np.where(data_mapping == atlas_roi, 0, data_mapping)\n",
    "            elif method == 'pvals':\n",
    "                data_mapping = np.where(data_mapping == atlas_roi, 1, data_mapping)\n",
    "\n",
    "        else:\n",
    "            data_mapping = np.where(data_mapping == atlas_roi, data_hemi.iloc[int(position[atlas_roi]),0], data_mapping)\n",
    "\n",
    "    \n",
    "\n",
    "    view = plotting.view_surf(fs_plot, data_mapping, threshold=None, symmetric_cmap=True, cmap='jet', bg_map=fs_sulc)\n",
    "    return(data_mapping, view, fs_plot, fs_sulc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of the quality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in the quality of fit in controls (as patients apre considered \"abnormal\" the fit would be inevitable worse, while the fit on controls reflects the quality of fit more appropriately)\n",
    "v1_qm = idp_concat_quality(os.path.join(target_dir, 'V1'), idp_ids, suffix = 'controlstest')\n",
    "v2_qm = idp_concat_quality(os.path.join(target_dir, 'V2'), idp_ids, suffix = 'controlstest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of idp's, we'll do it separately for visits and patients and controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ivisit in range(1,3):\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'Y_controlstest.txt', idp_ids, 'Y_controlstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'yhat_controlstest.txt', idp_ids, 'Yhat_controlstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'Z_controlstest.txt', idp_ids, 'Z_controlstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'ys2_controlstest.txt', idp_ids, 'Ys2_controlstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'Y_patientstest.txt', idp_ids, 'Y_patientstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'yhat_patientstest.txt', idp_ids, 'Yhat_patientstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'Z_patientstest.txt', idp_ids, 'Z_patientstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "    idp_concat(os.path.join(target_dir, 'V'+str(ivisit)), 'ys2_patientstest.txt', idp_ids, 'Ys2_patientstest_concat.txt', t_dir = os.path.join(target_dir, 'V'+str(ivisit)))\n",
    "\n",
    "    folder_to_delete = os.path.join(target_dir, 'V'+str(ivisit))\n",
    "    [shutil.rmtree(os.path.join(folder_to_delete, d)) for d in os.listdir(folder_to_delete) if os.path.isdir(os.path.join(folder_to_delete, d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put everything into a nice xarray structure, so that it's easy to navigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together the results of normative models\n",
    "for idataset in ['controls', 'patients']:\n",
    "    print(idataset)\n",
    "    for ivisit in range(1,3):\n",
    "        print(ivisit)\n",
    "        temp_v2 = xr.concat([\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'Y_'+idataset+'test_concat.txt'), sep=' ', index_col =0)[idp_ids], [('subject', pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'v'+str(ivisit)+'_'+idataset+'_test.csv'), sep=' ', index_col =0).index), ('roi', idp_ids)]),\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'Z_'+idataset+'test_concat.txt'), sep=' ', index_col =0)[idp_ids], [('subject', pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'v'+str(ivisit)+'_'+idataset+'_test.csv'), sep=' ', index_col =0).index), ('roi', idp_ids)]),\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'Yhat_'+idataset+'test_concat.txt'), sep=' ', index_col =0)[idp_ids], [('subject', pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'v'+str(ivisit)+'_'+idataset+'_test.csv'), sep=' ', index_col =0).index), ('roi', idp_ids)]),\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'Ys2_'+idataset+'test_concat.txt'), sep=' ', index_col =0)[idp_ids], [('subject', pd.read_csv(os.path.join(target_dir, 'V'+str(ivisit), 'v'+str(ivisit)+'_'+idataset+'_test.csv'), sep=' ', index_col =0).index), ('roi', idp_ids)]),\n",
    "                  ], \n",
    "                  pd.Index(['Y', 'Z', 'Yhat', 'S2'], name='features')\n",
    "                  )\n",
    "        if ivisit == 1:\n",
    "            temp_v1=temp_v2\n",
    "            \n",
    "        if (idataset == 'controls') & (ivisit==2):\n",
    "            xr_con = xr.concat([\n",
    "                temp_v1, temp_v2\n",
    "            ],\n",
    "            pd.Index(['V1', 'V2'], name = 'visit'))\n",
    "\n",
    "        if (idataset == 'patients') & (ivisit==2):\n",
    "            xr_pat = xr.concat([\n",
    "                temp_v1, temp_v2\n",
    "            ],\n",
    "            pd.Index(['V1', 'V2'], name = 'visit'))\n",
    "\n",
    "        #     xr_df = xr.concat([xr_con, xr_pat], pd.Index(['controls', 'patients'], name='group'))\n",
    "\n",
    "# Add clinics\n",
    "# Load subjects indicies directly from file to ensure they match with the results\n",
    "con_idx = pd.read_csv(os.path.join(target_dir, 'V1', 'v1_controls_test.csv'), sep=' ', index_col =0).index\n",
    "pat_idx = pd.read_csv(os.path.join(target_dir, 'V1', 'v1_patients_test.csv'), sep=' ', index_col =0).index\n",
    "con_clin = xr.concat([\n",
    "        xr.DataArray(v1_con[['age', 'sex', 'site']].loc[con_idx], [('subject', con_idx), ('covariates', ['age', 'sex', 'site'])]),\n",
    "        xr.DataArray(v2_con[['age', 'sex', 'site']].loc[con_idx], [('subject', con_idx), ('covariates', ['age', 'sex', 'site'])])\n",
    "        ],\n",
    "        pd.Index(['V1', 'V2'], name='visit'))\n",
    "pat_clin = xr.concat([\n",
    "        xr.DataArray(v1_pat[['age', 'sex', 'site']], [('subject', pat_idx), ('covariates', ['age', 'sex', 'site'])]),\n",
    "        xr.DataArray(v2_pat[['age', 'sex', 'site']], [('subject', pat_idx), ('covariates', ['age', 'sex', 'site'])])\n",
    "        ],\n",
    "        pd.Index(['V1', 'V2'], name='visit'))\n",
    "\n",
    "# Merge into two final structures\n",
    "xr_pat = xr.merge([xr_pat.to_dataset(name = 'neuroimaging'), pat_clin.to_dataset(name ='clinics')])\n",
    "xr_con = xr.merge([xr_con.to_dataset(name = 'neuroimaging'), con_clin.to_dataset(name ='clinics')])\n",
    "\n",
    "\n",
    "# The results here are the xr_pat and xr_con xarray structure where are all the results of cross-sectional normative models and clinics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we use the information to compute the Z-diff score\n",
    "### Step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First, we compute the variation in healthy controls\n",
    "2) Then we substract the two visits in patients\n",
    "3) Finally, compute the zdiff score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Estimate the variance in healthy controls\n",
    "con_sqrt = np.sqrt(\n",
    "        pd.DataFrame(\n",
    "    (\n",
    "            (xr_con.sel(visit='V2', features='Y').neuroimaging - xr_con.sel(visit='V2', features='Yhat').neuroimaging)\n",
    "            -\n",
    "            (xr_con.sel(visit='V1', features='Y').neuroimaging - xr_con.sel(visit='V1', features='Yhat').neuroimaging)\n",
    "\n",
    "    ).var(axis=0).to_pandas()\n",
    "    ).T\n",
    ")\n",
    "\n",
    "# 2) Substract the two patient visits\n",
    "pat_zdiff =(\n",
    "                (xr_pat.sel(visit='V2', features='Y').neuroimaging - xr_pat.sel(visit='V2', features='Yhat').neuroimaging)\n",
    "                -\n",
    "                (xr_pat.sel(visit='V1', features='Y').neuroimaging - xr_pat.sel(visit='V1', features='Yhat').neuroimaging)\n",
    "            ).to_pandas()\n",
    "\n",
    "# 3) Compute the zdiff score\n",
    "pat_zdiff = pat_zdiff.div(con_sqrt.squeeze(), axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, you can continue the analysis using the new z-diff scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following figure only serves as a sanity check that there is no obvious systematic mistake in the computations*\n",
    "\n",
    "*Check the ranges and empty values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(pat_zdiff, vmin=-1.96, vmax=1.96, cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistically test the median of z-diff scores against zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should be visible from the histogram is that the p-values across the whole brain are skewed, with higher proportions around zero. This usually indicates a presnece of an effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "import statsmodels.stats.multitest as multi\n",
    "\n",
    "# Create empty lists to store the results\n",
    "columns = []\n",
    "w_statistics = []\n",
    "p_values = []\n",
    "\n",
    "# Loop through each column in the DataFrame\n",
    "for column in pat_zdiff.columns:\n",
    "    # Perform the Wilcoxon signed-rank test for the column against zero\n",
    "    w_stat, p_value = wilcoxon(pat_zdiff[column], zero_method='zsplit')\n",
    "    \n",
    "    # Store the results in the lists\n",
    "    columns.append(column)\n",
    "    w_statistics.append(w_stat)\n",
    "    p_values.append(p_value)\n",
    "\n",
    "# Create a new DataFrame from the lists\n",
    "wilcoxon_results = pd.DataFrame({'IDP': columns, 'W-statistic': w_statistics, 'P-value': p_values})\n",
    "\n",
    "# Apply Benjamini-Hochberg correction to the p-values\n",
    "wilcoxon_results['P-value_corrected'] = multi.multipletests(wilcoxon_results['P-value'], method='fdr_bh')[1]\n",
    "wilcoxon_results = pd.DataFrame(wilcoxon_results)\n",
    "\n",
    "# Add a column with the average z-score so that we know the polarity of the effect\n",
    "wilcoxon_results = wilcoxon_results.merge(pd.DataFrame(pat_zdiff.mean(),columns=['mean']), left_on='IDP', right_index=True)\n",
    "wilcoxon_results['P-value_plot'] = wilcoxon_results['P-value_corrected'] * np.sign(wilcoxon_results['mean'])\n",
    "\n",
    "# Plot the histogram of all pvalues to get an idea of the general trend\n",
    "sns.histplot(data = wilcoxon_results, x='P-value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the significant regions\n",
    "wilcoxon_results.loc[wilcoxon_results['P-value_corrected']<0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the regions on a spatial map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn.plotting as plotting\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Prepare colormap for plotting\n",
    "poster_reds = ['#E9E9E9', '#FDECF2', '#FCD9E6', '#FAC6D9', '#F9B4CD', '#F7A1C0', '#F68EB4', '#F47BA7', '#F3689B', '#F1558E', '#F04282', '#EE2F75', '#ED1D69', '#D81159']\n",
    "poster_reds = ListedColormap(poster_reds, 'indexed')\n",
    "poster_reds_r = ListedColormap(poster_reds.colors[::-1])\n",
    "poster_blues_r = ['#218380', '#25938F', '#29A39F', '#2DB4AF', '#31C4BF', '#3BCEC9', '#5CD6D2', '#7CDEDB', '#9DE7E4', '#ADEBE9', '#BEEFED', '#CEF3F2', '#DEF7F6', '#E9E9E9']\n",
    "poster_blues_r = ListedColormap(poster_blues_r, 'indexed')\n",
    "poster_blues = ListedColormap(poster_blues_r.colors[::-1])\n",
    "\n",
    "poster_blue_pink_palette_normal = ListedColormap([*poster_blues.colors,*poster_reds_r.colors])\n",
    "\n",
    "# Plot the results for the left hemisphere\n",
    "data_mapping, view, fs_plot, fs_sulc = prepare_destrieux_plotting(wilcoxon_results.set_index('IDP')['P-value_plot'].to_frame(), hemi='l', method='counts')\n",
    "plotting.view_surf(fs_plot, data_mapping, threshold=None, cmap=poster_blue_pink_palette_normal, bg_map=fs_sulc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results for the right hemisphere\n",
    "\n",
    "data_mapping, view, fs_plot, fs_sulc = prepare_destrieux_plotting(wilcoxon_results.set_index('IDP')['P-value_plot'].to_frame(), hemi='r', method='counts')\n",
    "plotting.view_surf(fs_plot, data_mapping, threshold=None, cmap=poster_blue_pink_palette_normal, bg_map=fs_sulc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cas eyou don't have enough controls for both adaptation procedures, you can drop the site-specific adaptation\n",
    "\n",
    "**However!**\n",
    "- You can not use the cross-sectional analysis, they are definitely wrong. The site-adaptation is linear and it shifts all z-scores systematically. However, for the difference of V2-V1, this offset evens out, so this step is technically redundant. In that case, you can either have more controls for the healthy variance estimation, or split it again and use the remaining controls for a group-level comparison.\n",
    "\n",
    "**The way to do that**\n",
    "- The way to skip the site-specific adaptation requires a bit of cheating in the procedure -- you can pretend that your sample actually belongs to one of the datasets that were used for training the models -- in that case, the pretrained offset is going to be used to adjust the z-scores. However, because we're not interested in the z-scores, we can then continue only using the Y, Yhat and S2 values.\n",
    "\n",
    "- Below is the example on how to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "v1_pat = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_patients_v1.csv'), index_col=0)\n",
    "v2_pat = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_patients_v2.csv'), index_col=0)\n",
    "v1_pat.insert(v1_pat.columns.get_loc('sitenum')+1, 'visit', 'V1')\n",
    "v2_pat.insert(v2_pat.columns.get_loc('sitenum')+1, 'visit', 'V2')\n",
    "v1_pat.insert(v1_pat.columns.get_loc('sitenum')+1, 'class', 'patient')\n",
    "v2_pat.insert(v2_pat.columns.get_loc('sitenum')+1, 'class', 'patient')\n",
    "\n",
    "\n",
    "v1_con = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_controls_v1.csv'), index_col=0)\n",
    "v2_con = pd.read_csv(os.path.join(working_dir, 'data', 'NIMH_controls_v2.csv'), index_col=0)\n",
    "v1_con.insert(v1_con.columns.get_loc('sitenum')+1, 'visit', 'V1')\n",
    "v2_con.insert(v2_con.columns.get_loc('sitenum')+1, 'visit', 'V2')\n",
    "v1_con.insert(v1_con.columns.get_loc('sitenum')+1, 'class', 'control')\n",
    "v2_con.insert(v2_con.columns.get_loc('sitenum')+1, 'class', 'control')\n",
    "\n",
    "df = pd.concat([pd.concat([pd.concat([v1_pat,v2_pat], axis=0), v1_con]), v2_con])\n",
    "df['site'] = 'ABCD_01'\n",
    "df['sitenum'] = 1\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index':'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which data columns are covariates?\n",
    "covariates = ['age', 'sex']\n",
    "\n",
    "# Limits for cubic B-spline basis\n",
    "xmin = -5\n",
    "xmax = 100\n",
    "\n",
    "# Absolute Z threshold above which a sample is considered to be an outlier (without fitting any model)\n",
    "outlier_thresh = 7\n",
    "\n",
    "# load the site IDs from the pretrained data (if you decide to change the pretrained models, don;t forget to also change site ids here)\n",
    "with open(os.path.join(pretrained_dir, 'docs', 'site_ids_ct_82sites.txt')) as f:\n",
    "    site_ids_pretrained = f.read().splitlines()\n",
    "\n",
    "# load names from the Destrioux freesurfer parcellation atlas\n",
    "with open(os.path.join(pretrained_dir,'docs','phenotypes_ct_lh.txt')) as f:\n",
    "    idp_ids_lh = f.read().splitlines()\n",
    "with open(os.path.join(pretrained_dir,'docs','phenotypes_ct_rh.txt')) as f:\n",
    "    idp_ids_rh = f.read().splitlines()\n",
    "\n",
    "idp_ids = idp_ids_lh + idp_ids_rh\n",
    "idp_ids.remove('lh_MeanThickness_thickness')\n",
    "idp_ids.remove('rh_MeanThickness_thickness')\n",
    "\n",
    "# Which sites are present in our dataset?\n",
    "site_ids_test = df['site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# For sanity checks, save the files into *.csv\n",
    "df.to_csv(os.path.join(target_dir, 'all_data.csv'), sep=' ')\n",
    "\n",
    "###\n",
    "# The pretrained model coeffitients are stored individually for every IDP, so we need to cycle through them and apply the models\n",
    "###\n",
    "for iidp_no, iidp in enumerate(idp_ids):\n",
    "    # Create folder for the particular IDP (in appropriate visit dir)\n",
    "    idp_dir = os.path.join(target_dir, iidp)\n",
    "    os.makedirs(idp_dir, exist_ok=True)\n",
    "    os.chdir(idp_dir)\n",
    "    # Assuming that the data used were a part of the original dataset on which the model was trained\n",
    "    if all (elem in site_ids_pretrained for elem in site_ids_test):   \n",
    "        \n",
    "        print('The data are that are part of the original dataset')\n",
    "        \n",
    "        # Configure and save the design matrix\n",
    "        design_matrix = create_design_matrix(df[covariates],\n",
    "                                                site_ids = df['site'],\n",
    "                                                all_sites = site_ids_pretrained,\n",
    "                                                basis = 'bspline', xmin = xmin, xmax = xmax)\n",
    "        np.savetxt(os.path.join(idp_dir, 'design_matrix.txt'), design_matrix)\n",
    "\n",
    "        ###\n",
    "        # Save responses\n",
    "        ###\n",
    "        np.savetxt(os.path.join(idp_dir, 'response.txt'), df[iidp].to_numpy())\n",
    "\n",
    "        ###\n",
    "        # Save sitenum\n",
    "        ###\n",
    "        np.savetxt(os.path.join(idp_dir, 'sitenum.txt'), df['sitenum'].to_numpy())\n",
    "\n",
    "        ###\n",
    "        # Actually run the model\n",
    "        ###\n",
    "        yhat, s2, Z, y = predict(os.path.join(idp_dir, 'design_matrix.txt'),\n",
    "                                    respfile = os.path.join(idp_dir, 'response.txt'),\n",
    "                                    alg='blr',\n",
    "                                    model_path = os.path.join(pretrained_dir, 'models', 'lifespan_57K_82sites', iidp, 'Models'),\n",
    "                                    return_y = True\n",
    "                                    )\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From now on, you can continue as in the original tutorial, don't forget to disentangle visits and classes\n",
    "- you can check that if you use the same controls for the estimation of healthy variance (as in the original tutorial), the resulting z-diff scores of patients will be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the results across IDPs and remove the unnecessary folders\n",
    "idp_concat(target_dir, 'Y_predict.txt', idp_ids, 'Y_predict_concat.txt', t_dir = target_dir)\n",
    "idp_concat(target_dir, 'yhat_predict.txt', idp_ids, 'Yhat_predict_concat.txt', t_dir = target_dir)\n",
    "idp_concat(target_dir, 'Z_predict.txt', idp_ids, 'Z_predict_concat.txt', t_dir = target_dir)\n",
    "idp_concat(target_dir, 'ys2_predict.txt', idp_ids, 'Ys2_predict_concat.txt', t_dir = target_dir)\n",
    "\n",
    "[shutil.rmtree(os.path.join(target_dir, d)) for d in idp_ids if os.path.isdir(os.path.join(target_dir, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it easier, this is the code, which again concatenates data to xarray structures\n",
    "\n",
    "for iclass in ['patient', 'control']:\n",
    "    for ivisit in ['V1', 'V2']:\n",
    "\n",
    "        idc_filter = df[(df['class'] == iclass) & (df['visit'] == ivisit)].index.to_numpy()\n",
    "\n",
    "        temp_v2 = xr.concat([\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'Y_predict_concat.txt'), sep=' ', index_col =0)[idp_ids].loc[idc_filter], [('subject', df.loc[idc_filter]['ID']), ('roi', idp_ids)]),\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'Z_predict_concat.txt'), sep=' ', index_col =0)[idp_ids].loc[idc_filter], [('subject', df.loc[idc_filter]['ID']), ('roi', idp_ids)]),\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'Yhat_predict_concat.txt'), sep=' ', index_col =0)[idp_ids].loc[idc_filter], [('subject', df.loc[idc_filter]['ID']), ('roi', idp_ids)]),\n",
    "                    xr.DataArray(pd.read_csv(os.path.join(target_dir, 'Ys2_predict_concat.txt'), sep=' ', index_col =0)[idp_ids].loc[idc_filter], [('subject', df.loc[idc_filter]['ID']), ('roi', idp_ids)])\n",
    "                    ], \n",
    "                    pd.Index(['Y', 'Z', 'Yhat', 'S2'], name='features')\n",
    "                    )\n",
    "        \n",
    "        if ivisit == 'V1':\n",
    "            temp_v1=temp_v2\n",
    "        \n",
    "        if iclass == 'patient':\n",
    "            xr_pat2 = xr.concat([\n",
    "                temp_v1, temp_v2\n",
    "            ],\n",
    "            pd.Index(['V1', 'V2'], name = 'visit'))\n",
    "        \n",
    "        if iclass == 'control':\n",
    "            xr_con2 = xr.concat([\n",
    "                temp_v1, temp_v2\n",
    "            ],\n",
    "            pd.Index(['V1', 'V2'], name = 'visit'))\n",
    "\n",
    "con_clin2 = xr.concat([\n",
    "        xr.DataArray(df[['age', 'sex', 'site']].loc[np.where(df[(df['class'] == 'control') & (df['visit'] == 'V1')]['ID'])], [('subject', df[(df['class'] == 'control') & (df['visit'] == 'V1')]['ID']), ('covariates', ['age', 'sex', 'site'])]),\n",
    "        xr.DataArray(df[['age', 'sex', 'site']].loc[np.where(df[(df['class'] == 'control') & (df['visit'] == 'V2')]['ID'])], [('subject', df[(df['class'] == 'control') & (df['visit'] == 'V2')]['ID']), ('covariates', ['age', 'sex', 'site'])]),\n",
    "        ],\n",
    "        pd.Index(['V1', 'V2'], name='visit'))\n",
    "\n",
    "pat_clin2 = xr.concat([\n",
    "        xr.DataArray(df[['age', 'sex', 'site']].loc[np.where(df[(df['class'] == 'patient') & (df['visit'] == 'V1')]['ID'])], [('subject', df[(df['class'] == 'patient') & (df['visit'] == 'V1')]['ID']), ('covariates', ['age', 'sex', 'site'])]),\n",
    "        xr.DataArray(df[['age', 'sex', 'site']].loc[np.where(df[(df['class'] == 'patient') & (df['visit'] == 'V2')]['ID'])], [('subject', df[(df['class'] == 'patient') & (df['visit'] == 'V2')]['ID']), ('covariates', ['age', 'sex', 'site'])]),\n",
    "        ],\n",
    "        pd.Index(['V1', 'V2'], name='visit'))\n",
    "\n",
    "xr_pat2 = xr.merge([xr_pat2.to_dataset(name = 'neuroimaging'), pat_clin2.to_dataset(name ='clinics')])\n",
    "xr_con2 = xr.merge([xr_con2.to_dataset(name = 'neuroimaging'), con_clin2.to_dataset(name ='clinics')])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PCN_debug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
